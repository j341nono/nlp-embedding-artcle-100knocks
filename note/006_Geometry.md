### How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings
([EMNLP-2019](https://aclanthology.org/D19-1006/))

- 入力層以外の層から得た文脈表現は全て異方性を持つ．異方性はランダムに取得した単語の平均余弦類似度で算出できる．([Arora et al., 2017](https://openreview.net/pdf?id=SyK00v5xx))

- ELMo, BERT, GPT2 において，文脈を考慮していない入力層よりも，考慮した最終層付近の方が異方性の度合いが高いことから，異方性は文脈化のプロセスに内在しているといえる．

- ストップワードが最も文脈に応じて変化する表現だった．これは，単語が本来持っている多義性よりも単語が登場する文脈の多様さが，文脈化単語埋め込みの多様性を生み出しているといえる．このように，語義の少ない単語表現にこれほどばらつきが生じていることから，各単語に有限個の単語表現を割り当てているのではないと示唆される．

- ELMo は，上位層になるほど単語表現はより文脈依存的になり，文内類似度も向上した．文内類似度とは，各単語と平均ベクトルの余弦類似度から算出された値である．これは，ある文中の各単語は同じ文を共有しており，文脈から表現が形成されるという意味で，直感的には分布仮説を文レベルに拡張したものといえる．

- BERT は，上位層になるほど文内類似度が減少したが，文内類似度は平均余弦類似度よりも高い値となっていることから，周囲の単語が意味を形成しつつも，必ずしも同じ意味を持つ必要はないと認識しており，ELMo よりもよりニュアンスに富んだ文脈化を行っていると言える．

- GPT2 は，文内類似度が平均余弦類似度とほぼ同じであり，文脈化されていない入力層においてもっとも数値が高かった．ELMo や BERT においては，文脈化することは文内の任意の単語表現を類似させることであると主張できたが，GPT2 の結果から，そうではないといえる．

- MEV により，ある単語の分散に対して第一主成分で判別できる割合を算出したところ，どのモデルのどの層も平均して 5% 未満であり，静的な単語埋め込みでは動的な単語埋め込みを説明できないことを表している．このことから，BERT などは多義語として存在する数種類を使い分けているだけではないと示唆される．GPT2 は MEV の値が高かったが，これは高い異方性により一つの軸に押し込められているだけである．

- GloVe や FastText などの静的単語埋め込み，文脈化埋め込みモデルの第一主成分の埋め込みを使用し，単語レベルのタスクで実験したところ，後者の方が良い結果となった．また，文脈化埋め込みモデルの中でも，入力層付近の方が良い結果となった．また，より文脈化された表現を生成するGPT2 は他の2つよりも低い結果となったことも踏まえると，従来の単語レベルのタスクにおいて，第一主成分の文脈化埋め込みは重要ではないことがわかった．

