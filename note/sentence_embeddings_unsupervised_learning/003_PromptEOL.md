
### Scaling Sentence Embeddings with Large Language Models ([EMNLP-2024](https://aclanthology.org/2024.findings-emnlp.181/))

- LLM のパラメータが増加すればするほど性能が向上するわけではなかった理由として，パラメータ数が向上すると異方性の影響を強く受けることが挙げられる．(3183,1)

- LLM 版のプロンプトの選定として，まず sentence: "[text]" means を考え，文の最後のトークンの隠れ層の埋め込みを使用した．また，プロンプトを使用しない比較対照として，平均プーリングと文末トークンの利用を実験した．この比較において，色々なパラメータサイズで比較した．100M ~ 数十B の 8サイズ．(3184,3.1)
- Prompt の選定には STS-B の検証セットを利用した．(3184,3.1)
- PromptBERT のプロンプトの成功要因として，文末の . があることで，文全体の要約後が [MASK] に集約させることができたのではないか，ということでこれを削除して実験したところ，73.44 から 33.89 へと大幅に性能低下したことから，inplicit one word limitation の状態だった．したがって，LLM Ver. でも one word limitation を explicit する．This sentence: "[text]" means in one word:"(3184, 3.1)
- one-word limitation は文全体を一語で表現する，という意味ではなく，文全体を次のトークンの隠れ状態に凝縮し，その表現を文埋め込みとして利用する，ということである．(3184, 3.1)

- 追加訓練なしの設定において STS タスクではモデルサイズの向上に伴って性能が向上しなかった．そもそも，STSには以下の2つの基準が求められ，1つ目が分の意味的な情報をを保持すること，2つ目が埋め込み空間において似た意味は近くに配置されること，である．実際，モデルサイズが向上すると，2つ面お基準を満たさなくなる．これは，異方性とも関わっており，実際，モデルサイズの向上に伴って異方性も向上している．また，異方性の軽減に有効な対照学習を実施した場合はモデルサイズの向上に伴って性能が向上したことも由来する．(3189, 5.2)